{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"proj_tech_segmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sXg-JljaIiM","executionInfo":{"status":"ok","timestamp":1628666428606,"user_tz":-480,"elapsed":361187,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}},"outputId":"c113264d-40d5-4437-efc0-af17cf08c3ca"},"source":["# Connect to Google Colab VM, ignore this if execute locally\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/BraTs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m_WzC3_CZ9AT"},"source":["# CNN-based Brain Tumour Segmentation Network\n","## Import packages\n","Please make sure you have all the required packages installed. If GPU is available, but you want to use CPU to train your model, make sure you add \" os.environ['CUDA_VISIBLE_DEVICES'] = '-1'.\n","Package 'SimpleITK' is for loading the MR images, so you need to install it first."]},{"cell_type":"code","metadata":{"id":"l2o_B133Z9AU","executionInfo":{"status":"ok","timestamp":1628666848455,"user_tz":-480,"elapsed":3471,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}}},"source":["import numpy as np \n","from tqdm import tqdm\n","import os\n","import random\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","import shutil\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import *\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.compat.v1 import ConfigProto\n","from tensorflow.compat.v1 import InteractiveSession\n","from keras import backend as K\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","config = ConfigProto()\n","config.gpu_options.allow_growth = True\n","session = InteractiveSession(config=config)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x-idvn2KQFVQ"},"source":["## Divide into Training and Validation Set"]},{"cell_type":"code","metadata":{"id":"GlAqIWNMZ9AW","executionInfo":{"status":"ok","timestamp":1628669031528,"user_tz":-480,"elapsed":1451173,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}}},"source":["# uncomment if using linux/macos\n","!rm -rf Train Val\n","!mkdir Train Val Train/Yes Train/No Val/Yes Val/No\n","\n","# uncomment if using windows\n","# !rmdir Train Val /s /q\n","# !md Train Val Train\\Yes Train\\No Val\\Yes Val\\No\n","\n","img_path = '/content/drive/MyDrive/Dataset/'\n","train_list = []\n","val_list = []\n","for CLASS in os.listdir(img_path):\n","    if not CLASS.startswith('.'):\n","        all_files = os.listdir(img_path + CLASS)\n","        files = [item for item in all_files if \"img\" in item]\n","        random.shuffle(files)\n","        img_num = len(files)\n","        for (n, file_name) in enumerate(files):\n","            img = os.path.join(img_path,CLASS,file_name)\n","            seg = os.path.join(img_path,CLASS,file_name.split('_')[0]+'_seg.npy')\n","            # 80% of images will be used for training, change the number here \n","            # to use different number of images for training your model.\n","            if n < 0.8*img_num:\n","                shutil.copy(img, os.path.join('Train/',CLASS,file_name))\n","                shutil.copy(seg, os.path.join('Train/',CLASS,file_name.split('_')[0]+'_seg.npy'))\n","            else:\n","                shutil.copy(img, os.path.join('Val/',CLASS,file_name))\n","                shutil.copy(seg, os.path.join('Val/',CLASS,file_name.split('_')[0]+'_seg.npy'))\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wu_u4yvZ9AX"},"source":["## Train-time data augmentation\n","Generalizability is crucial to a deep learning model and it refers to the performance difference of a model when evaluated on the seen data (training data) versus the unseen data (testing data). Improving the generalizability of these models has always been a difficult challenge. \n","\n","**Data Augmentation** is an effective way of improving the generalizability, because the augmented data will represent a more comprehensive set of possible data samples and minimizing the distance between the training and validation/testing sets.\n","\n","There are many data augmentation methods you can choose in this projects including rotation, shifting, flipping, etc.\n","\n","You are encouraged to try different augmentation method to get the best segmentation result.\n","\n","\n","## Get the data generator ready"]},{"cell_type":"code","metadata":{"id":"gInItCo9Z9AX","executionInfo":{"status":"ok","timestamp":1628669038831,"user_tz":-480,"elapsed":277,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}}},"source":["# Get the data generator ready\n","class DataGenerator(tf.keras.utils.Sequence):\n","    'Generates data for Keras'\n","    def __init__(self, list_IDs, batch_size=16, dim=(240,240), n_channels=3,\n","                    n_classes=2, shuffle=True):\n","        'Initialization'\n","        self.dim = dim\n","        self.batch_size = batch_size\n","        self.list_IDs = list_IDs\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.shuffle = shuffle\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        'Denotes the number of batches per epoch'\n","        return int(np.floor(len(self.list_IDs) / self.batch_size))\n","\n","    def __getitem__(self, index):\n","        'Generate one batch of data'\n","        # Generate indexes of the batch\n","        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n","\n","        # Find list of IDs\n","        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n","\n","        # Generate data\n","        X, Y = self.__data_generation(list_IDs_temp)\n","\n","        return X, Y\n","\n","    def on_epoch_end(self):\n","        'Updates indexes after each epoch'\n","        self.indexes = np.arange(len(self.list_IDs))\n","        if self.shuffle == True:\n","            np.random.shuffle(self.indexes)\n","\n","    def __data_generation(self, list_IDs_temp):\n","        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n","        # Initialization\n","        X = np.empty((self.batch_size, *self.dim, 1))\n","        Y = np.empty((self.batch_size, *self.dim, 2))\n","\n","        datagen = ImageDataGenerator(horizontal_flip=True,\n","                    vertical_flip=True,\n","                    rotation_range=15,\n","                    zoom_range=0.05)\n","\n","        # Generate data\n","        for i, ID in enumerate(list_IDs_temp):\n","            # Store sample\n","            # Add data augmentation here\n","            img = np.load(ID)[:,:,0]\n","            img = np.expand_dims(img, axis=2)\n","\n","            X[i,] = img\n","\n","            # Store class\n","            label = np.load(ID.split('_')[0]+'_seg.npy')\n","            label = np.expand_dims(label, axis=2)\n","            label = np.concatenate(((-label)+1, label),axis=-1)\n","            Y[i,] = label\n","\n","        gen_data = datagen.flow(X, Y, batch_size=16)\n","        return gen_data.x, gen_data.y"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WNRSW_n_c8pE","executionInfo":{"status":"ok","timestamp":1628669041783,"user_tz":-480,"elapsed":442,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}},"outputId":"c0e9ebfc-23c5-4281-ab43-035f7d77cd91"},"source":["train_list = []\n","val_list = []\n","\n","train_root = '/content/drive/My Drive/BraTs/Train/Yes'\n","for path in os.listdir(train_root):\n","  if path.split(\".\")[0].split(\"_\")[1] == \"img\":\n","    train_list.append(os.path.join(train_root, path))\n","train_root = '/content/drive/My Drive/BraTs/Train/No'\n","for path in os.listdir(train_root):\n","  if path.split(\".\")[0].split(\"_\")[1] == \"img\":\n","    train_list.append(os.path.join(train_root, path))\n","val_root = '/content/drive/My Drive/BraTs/Val/Yes'\n","for path in os.listdir(val_root):\n","  if path.split(\".\")[0].split(\"_\")[1] == \"img\":\n","    val_list.append(os.path.join(val_root, path))\n","val_root = '/content/drive/My Drive/BraTs/Val/No'\n","for path in os.listdir(val_root):\n","  if path.split(\".\")[0].split(\"_\")[1] == \"img\":\n","    val_list.append(os.path.join(val_root, path))\n","\n","print(len(train_list)) # 4377\n","print(len(val_list))  # 1077\n","\n","train_generator = DataGenerator(train_list)\n","validation_generator = DataGenerator(val_list)\n","img_size = (240,240)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["4314\n","1077\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QcV0Kd3sZ9AY"},"source":["## Define a metric for the performance of the model\n","Dice score is used here to evaluate the performance of your model.\n","More details about the Dice score and other metrics can be found at \n","https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2. Dice score can be also used as the loss function for training your model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Hdwx0iJeZ9AY","executionInfo":{"status":"ok","timestamp":1628669049095,"user_tz":-480,"elapsed":487,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}},"outputId":"7b51af42-2fb6-4ae2-aefe-bc78cbf46590"},"source":["def dice_coef(y_true, y_pred, smooth=1.0):\n","    class_num = 2\n","    for i in range(class_num):\n","        y_true_f = K.flatten(y_true[:,:,:,i])\n","        y_pred_f = K.flatten(y_pred[:,:,:,i])\n","        intersection = K.sum(y_true_f * y_pred_f)\n","        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n","        if i == 0:\n","            total_loss = loss\n","        else:\n","            total_loss = total_loss + loss\n","    total_loss = total_loss / class_num\n","    return total_loss\n","\n","\n","def dice_coef_loss(y_true, y_pred):\n","    return 1-dice_coef(y_true, y_pred)\n","\n","def tversky(y_true, y_pred, alpha=0.7):\n","    smooth = 1\n","    class_num = 2\n","    for i in range (class_num):  \n","        y_true_pos = K.flatten(y_true[:,:,:,i])\n","        y_pred_pos = K.flatten(y_pred[:,:,:,i])\n","        true_pos = K.sum(y_true_pos * y_pred_pos)\n","        false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n","        false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n","        loss = (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n","        if i == 0:\n","            total_loss = loss\n","        else:\n","            total_loss = total_loss + loss\n","    total_loss = total_loss / class_num\n","    return total_loss\n","\n","def tversky_loss(y_true, y_pred):\n","    return 1 - tversky(y_true,y_pred)\n","\n","def focal_tversky(y_true,y_pred):\n","    pt_1 = tversky(y_true, y_pred)\n","    gamma = 0.75\n","    return K.pow((1-pt_1), gamma)\n","\n","def log_cosh_dice_loss(y_true, y_pred):\n","        x = dice_coef_loss(y_true, y_pred)\n","        return tf.math.log((tf.exp(x) + tf.exp(-x)) / 2.0)\n"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'def focal_dice_loss(y_true,y_pred):\\n    alpha = 0.25\\n    return alpha * focal_loss(y_true,y_pred) - \\n    K.log(dice_coef_loss(y_true,y_pred))'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"nXxxYKAMZ9AZ"},"source":["## Build your own model here"]},{"cell_type":"markdown","metadata":{"id":"PAEmFi0XEF9A"},"source":["###Res-Unet Architecture\n","https://arxiv.org/pdf/1711.10684.pdf"]},{"cell_type":"code","metadata":{"id":"9TkV0IbBKbU3","executionInfo":{"status":"ok","timestamp":1628669051986,"user_tz":-480,"elapsed":2,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}}},"source":["class ResBlock(Layer):\n","    \"\"\"\n","    Represents the Residual Block in the ResUNet architecture.\n","    \"\"\"\n","    def __init__(self, filters, strides, **kwargs):\n","        super(ResBlock, self).__init__(**kwargs)\n","        self.filters = filters\n","        self.strides = strides\n","\n","        self.bn1 = BatchNormalization()\n","        self.relu1 = ReLU()\n","        self.conv1 = Conv2D(filters=filters, kernel_size=3, strides=strides, padding=\"same\", use_bias=False)\n","\n","        self.bn2 = BatchNormalization()\n","        self.relu2 = ReLU()\n","        self.conv2 = Conv2D(filters=filters, kernel_size=3, strides=1, padding=\"same\", use_bias=False)\n","\n","        self.conv_skip = Conv2D(filters=filters, kernel_size=1, strides=strides, padding=\"same\", use_bias=False)\n","        self.bn_skip = BatchNormalization()\n","\n","        self.add = Add()\n","\n","    def call(self, inputs, training=False, **kwargs):\n","        x = inputs\n","        x = self.bn1(x, training=training)\n","        x = self.relu1(x)\n","        x = self.conv1(x)\n","\n","        x = self.bn2(x, training=training)\n","        x = self.relu2(x)\n","        x = self.conv2(x)\n","\n","        skip = self.conv_skip(inputs)\n","        skip = self.bn_skip(skip, training=training)\n","\n","        res = self.add([x, skip])\n","        return res"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"x3MKO6k1KS8T","executionInfo":{"status":"ok","timestamp":1628669054258,"user_tz":-480,"elapsed":302,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}}},"source":["def ResUNet(input_shape, classes: int, filters_root: int = 64, depth: int = 3):\n","\n","    input = Input(shape=input_shape)\n","\n","    layer = input\n","\n","    # ENCODER\n","    encoder_blocks = []\n","\n","    filters = filters_root\n","    layer = Conv2D(filters=filters, kernel_size=3, strides=1, padding=\"same\")(layer)\n","\n","    branch = Conv2D(filters=filters, kernel_size=3, strides=1, padding=\"same\", use_bias=False)(layer)\n","    branch = BatchNormalization()(branch)\n","    branch = ReLU()(branch)\n","    branch = Conv2D(filters=filters, kernel_size=3, strides=1, padding=\"same\", use_bias=True)(branch)\n","    layer = Add()([branch, layer])\n","\n","    encoder_blocks.append(layer)\n","\n","    for _ in range(depth - 1):\n","        filters *= 2\n","        layer = ResBlock(filters, strides=2)(layer)\n","\n","        encoder_blocks.append(layer)\n","\n","    # BRIDGE\n","    filters *= 2\n","    layer = ResBlock(filters, strides=2)(layer)\n","\n","    # DECODER\n","    for i in range(1, depth + 1):\n","        filters //= 2\n","        skip_block_connection = encoder_blocks[-i]\n","\n","        layer = UpSampling2D()(layer)\n","        layer = Concatenate()([layer, skip_block_connection])\n","        layer = ResBlock(filters, strides=1)(layer)\n","\n","    layer = Conv2D(filters=classes, kernel_size=1, strides=1, padding=\"same\")(layer)\n","    layer = Softmax()(layer)\n","\n","    output = layer\n","\n","    return Model(input, output)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vdQHqtQaH9ya","executionInfo":{"status":"ok","timestamp":1628669058626,"user_tz":-480,"elapsed":1163,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}},"outputId":"514d7eeb-980c-45ba-a1ed-33bb1b2e72fe"},"source":["import keras\n","\n","model = ResUNet((240,240,1),2)\n","\n","try:\n","    lr = args.lr\n","except:\n","    lr = 1e-3\n","model.compile(optimizer=Adam(lr=lr), loss=log_cosh_dice_loss, metrics=[dice_coef])\n","model.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 240, 240, 1) 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 240, 240, 64) 640         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 240, 240, 64) 36864       conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 240, 240, 64) 256         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","re_lu (ReLU)                    (None, 240, 240, 64) 0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 240, 240, 64) 36928       re_lu[0][0]                      \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 240, 240, 64) 0           conv2d_2[0][0]                   \n","                                                                 conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","res_block (ResBlock)            (None, 120, 120, 128 230656      add[0][0]                        \n","__________________________________________________________________________________________________\n","res_block_1 (ResBlock)          (None, 60, 60, 256)  920064      res_block[0][0]                  \n","__________________________________________________________________________________________________\n","res_block_2 (ResBlock)          (None, 30, 30, 512)  3675136     res_block_1[0][0]                \n","__________________________________________________________________________________________________\n","up_sampling2d (UpSampling2D)    (None, 60, 60, 512)  0           res_block_2[0][0]                \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 60, 60, 768)  0           up_sampling2d[0][0]              \n","                                                                 res_block_1[0][0]                \n","__________________________________________________________________________________________________\n","res_block_3 (ResBlock)          (None, 60, 60, 256)  2561024     concatenate[0][0]                \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 120, 120, 256 0           res_block_3[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 120, 120, 384 0           up_sampling2d_1[0][0]            \n","                                                                 res_block[0][0]                  \n","__________________________________________________________________________________________________\n","res_block_4 (ResBlock)          (None, 120, 120, 128 641536      concatenate_1[0][0]              \n","__________________________________________________________________________________________________\n","up_sampling2d_2 (UpSampling2D)  (None, 240, 240, 128 0           res_block_4[0][0]                \n","__________________________________________________________________________________________________\n","concatenate_2 (Concatenate)     (None, 240, 240, 192 0           up_sampling2d_2[0][0]            \n","                                                                 add[0][0]                        \n","__________________________________________________________________________________________________\n","res_block_5 (ResBlock)          (None, 240, 240, 64) 161024      concatenate_2[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_21 (Conv2D)              (None, 240, 240, 2)  130         res_block_5[0][0]                \n","__________________________________________________________________________________________________\n","softmax (Softmax)               (None, 240, 240, 2)  0           conv2d_21[0][0]                  \n","==================================================================================================\n","Total params: 8,264,258\n","Trainable params: 8,255,170\n","Non-trainable params: 9,088\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"HTU08F8DTGjo"},"source":["## Cyclic LR\n","Set the learning rate dynamically, can search many local minimum and have a better perfomance.\n","\n","https://arxiv.org/pdf/1506.01186.pdf"]},{"cell_type":"code","metadata":{"id":"LZPFLOfgTF9x","executionInfo":{"status":"ok","timestamp":1628669063897,"user_tz":-480,"elapsed":295,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}}},"source":["from tensorflow.keras.callbacks import *\n","from tensorflow.keras import backend as K\n","import numpy as np\n","\n","class CyclicLR(Callback):\n","    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n","                 gamma=1., scale_fn=None, scale_mode='cycle'):\n","        super(CyclicLR, self).__init__()\n","\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.step_size = step_size\n","        self.mode = mode\n","        self.gamma = gamma\n","        if scale_fn == None:\n","            if self.mode == 'triangular':\n","                self.scale_fn = lambda x: 1.\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'triangular2':\n","                self.scale_fn = lambda x: 1/(2.**(x-1))\n","                self.scale_mode = 'cycle'\n","            elif self.mode == 'exp_range':\n","                self.scale_fn = lambda x: gamma**(x)\n","                self.scale_mode = 'iterations'\n","        else:\n","            self.scale_fn = scale_fn\n","            self.scale_mode = scale_mode\n","        self.clr_iterations = 0.\n","        self.trn_iterations = 0.\n","        self.history = {}\n","\n","        self._reset()\n","\n","    def _reset(self, new_base_lr=None, new_max_lr=None,\n","               new_step_size=None):\n","        \"\"\"Resets cycle iterations.\n","        Optional boundary/step size adjustment.\n","        \"\"\"\n","        if new_base_lr != None:\n","            self.base_lr = new_base_lr\n","        if new_max_lr != None:\n","            self.max_lr = new_max_lr\n","        if new_step_size != None:\n","            self.step_size = new_step_size\n","        self.clr_iterations = 0.\n","        \n","    def clr(self):\n","        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n","        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n","        if self.scale_mode == 'cycle':\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n","        else:\n","            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n","        \n","    def on_train_begin(self, logs={}):\n","        logs = logs or {}\n","\n","        if self.clr_iterations == 0:\n","            K.set_value(self.model.optimizer.lr, self.base_lr)\n","        else:\n","            K.set_value(self.model.optimizer.lr, self.clr())        \n","            \n","    def on_batch_end(self, epoch, logs=None):\n","        \n","        logs = logs or {}\n","        self.trn_iterations += 1\n","        self.clr_iterations += 1\n","\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        self.history.setdefault('iterations', []).append(self.trn_iterations)\n","\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","        \n","        K.set_value(self.model.optimizer.lr, self.clr())"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5y3BpQwUl6H","executionInfo":{"status":"ok","timestamp":1628669067101,"user_tz":-480,"elapsed":278,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}}},"source":["clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n","            step_size=400., mode='exp_range', gamma=0.99994)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sanJWVwHZ9AZ"},"source":["## Train your model here\n","Once you defined the model and data generator, you can start training your model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"DAdm2TCHZ9Aa","executionInfo":{"status":"error","timestamp":1628669249312,"user_tz":-480,"elapsed":121694,"user":{"displayName":"Zhang Ce","photoUrl":"","userId":"17377411366687565887"}},"outputId":"f9badbdd-3642-4d97-d30a-801390831b48"},"source":["from keras import callbacks\n","\n","num_epochs = 10\n","model.fit_generator(train_generator, epochs=num_epochs,\n","                validation_data=validation_generator, workers=16, callbacks=[clr])"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-3cac0f9891c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit_generator(train_generator, epochs=num_epochs,\n\u001b[0;32m----> 5\u001b[0;31m                 validation_data=validation_generator, workers=16, callbacks=[clr])\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m   def evaluate_generator(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"qccCasBFZ9Aa"},"source":["## Save the model\n","Once your model is trained, remember to save it for testing."]},{"cell_type":"code","metadata":{"id":"U09B96rRZ9Aa"},"source":["model.save_weights('Res-Unet_weights.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m5lHIYYaZ9Aa"},"source":["## Run the model on the test set\n","After your last Q&A session, you will be given the test set. Run your model on the test set to get the segmentation results and submit your results in a .zip file. If the MRI image is named '100_img.npy', save your segmentation result as '100_seg.npy'. "]}]}